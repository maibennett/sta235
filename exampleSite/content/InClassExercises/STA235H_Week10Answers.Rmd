---
title: "Week 10 - In-Class Exercise"
author: "STA 235H - Fall 2022"
date: ""
output: 
  html_document:
    css: style.css
    df_print: "kable"
---
<style type="text/css">
div.main-container {
  max-width: 1800px;
  margin-left: auto;
  margin-right: auto;
}

@media only screen and (max-width: 600px) {
   body {
      margin: 0;
      padding: 0;
   }
   .sample {
      width: 100%;
   }
}
</style>

# Setup

You work at a marketing firm, and are trying to predict characteristics for different segments of your audience (customers from a car insurance company). You have the following dataset that will help you build your model:


```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(caret)
library(modelr)

marketing = read.csv("https://raw.githubusercontent.com/maibennett/sta235/main/exampleSite/content/Classes/Week10/1_ModelSelection/data/marketing.csv")

# For simplicity, we will drop some variables, but you can use them all afterward if you want!
marketing = marketing %>% select(Customer, Customer.Lifetime.Value, Response, 
                                  Coverage, Education, EmploymentStatus,
                                  Gender, Income, Location.Code, Marital.Status, 
                                  Monthly.Premium.Auto, 
                                  Months.Since.Last.Claim, Number.of.Policies,
                                  Policy.Type, Sales.Channel, Total.Claim.Amount, 
                                  Renew.Offer.Type)
```

Here's the list of some variables in your dataset:

- `Customer`: Customer ID (*Note: ID variables are usually not predictors!*)
- `Customer.Lifetime.Value`: Estimate of the value of a customer (*In this example, this is what we will predict*)
- `Response`: Whether the client took the offer or not
- `Coverage`: Type of insurance coverage
- `Education`, `Gender`, `Income`, `Location.Code`, `Marital.Status`: Client's characteristics.
- `Renew.Offer.Type`: Offer made to the client (out of four different options)

## Tasks

1) Let's prepare the data. Drop `Customer` from your dataset (we will not use it for prediction), and transform `Response` into a binary variable (1 if Response is Yes and 0 in another case).

**Q3: How many observations with Response = 1 do you have? And with Response = 0?**


```{r}
marketing = marketing %>% select(-Customer) %>% mutate(Response = ifelse(Response=="No", 0, 1))

marketing %>% select(Response) %>% table()
```

2) We will also need to transform our predictors into factor variables, when needed (prediction models tend not to like character variables). Use the following code and look at your data. What happened here?

```{r}
#head(marketing)

marketing = marketing %>% mutate_if(is.character, as.factor)
```

What we can see here is that all character variables are now transformed to categorical variables (factor variables). This will allow us to avoid problems if one of the methods we use has issues with character variables.


3) Now we are ready to roll. Using `set.seed(100)`, create a training dataset that is 75% of your data, and a testing dataset that is 25% of your data. I'm giving you the start of your code...

**Q4: How many observations does your testing dataset have?**

```{r}
set.seed(100) #Remember always to set seed before something random (in this case, sample())

n = nrow(marketing) #Number of obs in our marketing dataset

train = sample(1:n, nrow(marketing)*0.75) #Row numbers for 75% of our data (randomly selected)

train.data = marketing %>% slice(train)
test.data = marketing %>% slice(-train)

nrow(test.data) #Number of rows (observations) in our training dataset
```

4) Run a linear regression using all your covariates as predictors and `Customer.Lifetime.Value` as the outcome (make sure you exlcude `Response` as well, because that will also be an outcome that we will use next class). Fit your model on your training dataset, and get the RMSE on the testing dataset:

**Q5: What is the RMSE of the model on the testing dataset?**

```{r}
# When we use y ~ ., we are basically telling the model to use all the covariates in our dataset (without having to type it)

lm_clv = lm(Customer.Lifetime.Value ~ . - Response, data = train.data)

rmse(lm_clv, test.data)
```

5) Run the same model, but now use a cross-validation approach with 5-folds. In this case, you can use the entire dataset for this.

**Q6: Is your RMSE better with this approach or the last one?**

```{r}
set.seed(100)

train.control = trainControl(method = "cv", number = 5)

lm_clv_cv = train(Customer.Lifetime.Value ~ . - Response, 
                  data = marketing, method="lm", 
                  trControl = train.control)

# Look at the RMSE here!
lm_clv_cv
```

We can see that the RMSE here is actually a bit higher than before (that's ok!). Still remember that this one uses the entire dataset for training and testing, so it's probably more accurate.


If we want to look at the models selected by each method, we can also see it here:

```{r}
# If we want to compare both models, we can do it like this:

# These are the coefficients (beta's) for our linear model without cross validation
coef(lm_clv)
# These are the coefficients (beta's) for our linear model with cross-validation!
coef(lm_clv_cv$finalModel)
```

6) Finally, let's do a (forward) Stepwise selection model (using again a 5-fold CV). Remember that you need to identify how many predictors we have in this case (and factor variables count for more than 1 depending on the number of levels!):

```{r}
set.seed(100)

nvars = length(lm_clv_cv$coefnames)
  
train.control = trainControl(method = "cv", number = 5) #set up a 5-fold cv

lm.fwd = train(Customer.Lifetime.Value ~ . - Response, 
               data = train.data, 
               method = "leapForward",
               tuneGrid = data.frame(nvmax = 1:nvars), 
               trControl = train.control)

lm.fwd$bestTune #This is the number of variables we will be using

lm.fwd

# If we want to recover the coefficient names, we can use the coef() function:
coef(lm.fwd$finalModel, lm.fwd$bestTune$nvmax)

# If we want to get the RMSE:
rmse(lm.fwd, test.data)

```
