preProcess = c("center","scale"),
tuneLength = 25 #<<
)
knn$bestTune
plot(knn)
library(caret)
d <- read.csv("https://raw.githubusercontent.com/maibennett/sta235/main/exampleSite/content/Classes/Week11/2_KNearest/data/purchases_type.csv")
set.seed(100)
n <- nrow(d)
train.row <- sample(1:n, 0.8*n)
test.data <- d[-train.row,]
train.data <- d[train.row,]
knn <- train(
spend ~. - type, data = train.data,
method = "knn",
trControl = trainControl("cv", number = 10),
preProcess = c("center","scale"),
tuneLength = 15 #<<
)
library(caret)
d <- read.csv("https://raw.githubusercontent.com/maibennett/sta235/main/exampleSite/content/Classes/Week11/2_KNearest/data/purchases_type.csv")
set.seed(100)
n <- nrow(d)
train.row <- sample(1:n, 0.8*n)
test.data <- d[-train.row,]
train.data <- d[train.row,]
knnr <- train(
spend ~. - type, data = train.data,
method = "knn",
trControl = trainControl("cv", number = 10),
preProcess = c("center","scale"),
tuneLength = 15 #<<
)
plot(knnr)
knnr$bestTune
library(caret)
d <- read.csv("https://raw.githubusercontent.com/maibennett/sta235/main/exampleSite/content/Classes/Week11/2_KNearest/data/purchases_type.csv")
set.seed(100)
n <- nrow(d)
train.row <- sample(1:n, 0.8*n)
test.data <- d[-train.row,]
train.data <- d[train.row,]
knnr <- train(
spend ~. - type, data = train.data, #<<
method = "knn",
trControl = trainControl("cv", number = 10),
preProcess = c("center","scale"),
tuneLength = 25
)
plot(knnr)
library(caret)
d <- read.csv("https://raw.githubusercontent.com/maibennett/sta235/main/exampleSite/content/Classes/Week11/2_KNearest/data/purchases_type.csv")
set.seed(100)
n <- nrow(d)
train.row <- sample(1:n, 0.8*n)
test.data <- d[-train.row,]
train.data <- d[train.row,]
knnr <- train(
spend ~. - type, data = train.data, #<<
method = "knn",
trControl = trainControl("cv", number = 10),
preProcess = c("center","scale"),
tuneLength = 35
)
library(caret)
d <- read.csv("https://raw.githubusercontent.com/maibennett/sta235/main/exampleSite/content/Classes/Week11/2_KNearest/data/purchases_type.csv")
set.seed(100)
n <- nrow(d)
train.row <- sample(1:n, 0.8*n)
test.data <- d[-train.row,]
train.data <- d[train.row,]
knnr <- train(
spend ~. - type, data = train.data, #<<
method = "knn",
trControl = trainControl("cv", number = 10),
preProcess = c("center","scale"),
tuneLength = 35
)
plot(knnr)
library(caret)
d <- read.csv("https://raw.githubusercontent.com/maibennett/sta235/main/exampleSite/content/Classes/Week11/2_KNearest/data/purchases_type.csv")
set.seed(100)
n <- nrow(d)
train.row <- sample(1:n, 0.8*n)
test.data <- d[-train.row,]
train.data <- d[train.row,]
knnr <- train(
spend ~. - type, data = train.data, #<<
method = "knn",
trControl = trainControl("cv", number = 10),
preProcess = c("center","scale"),
tuneLength = 100
)
plot(knnr)
library(caret)
d <- read.csv("https://raw.githubusercontent.com/maibennett/sta235/main/exampleSite/content/Classes/Week11/2_KNearest/data/purchases_type.csv")
set.seed(100)
n <- nrow(d)
train.row <- sample(1:n, 0.8*n)
test.data <- d[-train.row,]
train.data <- d[train.row,]
knnr <- train(
spend ~. - type, data = train.data, #<<
method = "knn",
trControl = trainControl("cv", number = 10),
preProcess = c("center","scale"),
tuneLength = 50
)
plot(knnr)
knnr$bestTune
pagedown::chrome_print('C:/Users/mc72574/Dropbox/Hugo/Sites/sta235/exampleSite/content/Classes/Week11/2_KNearest/sp2021_sta235_14_knn.html', timeout = 10)
data <- read.csv("https://raw.githubusercontent.com/maibennett/sta235/main/exampleSite/content/Classes/Week11/1_Shrinkage/data/purchases.csv") # Load data
set.seed(100) # Remember to set a seed!
train.row <- sample(1:n, 0.8*n) # We generate a random sample for the training data
test.data <- data[-train.row,] #Select testing data
train.data <- data[train.row,] #Select training data
## Exercise: Play around with the proportions. What happens if you hold-out just 10% of the sample? (i.e. use only 10% of the sample for testing). How do your results change?
lambda_seq <- c(0,10^seq(-3, 3, length = 100)) # Create a vector for different values of lambda
## Question: Why do we use an exponential vector?
cv <- train(spend ~., data = train.data, # We give the function `train` the formula (`spend` regressed on everything else that's on the dataset). Note: If there are variables *you don't want to include* (e.g. an id variable), you can exlude it with a - sign (e.g. spend ~ . - id).
method = "glmnet", # We will be using an "Elastic Net" method (which is basically the method family for ridge and lasso reg)
preProcess = "scale", # Pre-process the data (remember that lasso and ridge are affected by scales!)
trControl = trainControl("cv", number = 10), # We are using a cross-validation method, with 10 folds. What happens if you change the number of folds?
tuneGrid = expand.grid(alpha = 0, # alpha is the "mixing" parameter. All you need to know is that `alpha=0` is for ridge regression (and `alpha=1` is for lasso)
lambda = lambda_seq) # values of lambda we are going to test out
)
lambda <- cv$results$lambda #We can extract the vector of tested lambas here
rmse <- cv$results$RMSE # We can extract the C-V RMSE for each of those lambdas!
## Exercise: What other important things do you see in the matrix `results`? Explore it!
# I create a new dataframe to be able to plot lambda against RMSE
cv_lambda <- data.frame(lambda = lambda,
rmse = rmse)
# Plot lambda vs RMSE
ggplot(data = cv_lambda, aes(x = lambda, y = rmse)) +
geom_line(aes(color = "#BF3984"), lwd = 2) +
geom_vline(aes(xintercept = cv$bestTune$lambda, color = "dark grey"), lty=2, lwd = 1.5) +
scale_color_manual(values=c("#BF3984","dark grey"),labels = c("#BF3984" = "CV-RMSE","dark grey" = "Min \u03BB")) +
theme_bw()+
xlab("\u03BB") + ylab("RMSE")+
theme(plot.margin=unit(c(1,1,1.5,1.2),"cm"),
panel.grid.major.x = element_blank(),
panel.grid.minor.x = element_blank(),
panel.grid.major.y = element_blank(),
panel.grid.minor.y = element_blank(),
axis.line = element_line(colour = "dark grey"))+
theme(axis.title.x = element_text(size=18),#margin = margin(t = 10, r = 0, b = 0, l = 0)),
axis.text.x = element_text(size=14),
axis.title.y = element_text(size=18),#margin = margin(t = 0, r = 10, b = 0, l = 0)),
axis.text.y = element_text(size=14),legend.position=c(0.9,0.1),
legend.title = element_blank(),
legend.text = element_text(size=15),
legend.background = element_rect(fill="white",colour ="dark grey"),
title = element_text(size=14))
cv$finalModel
pred.ridge <- cv %>% predict(test.data)
data.frame(
RMSE = RMSE(pred.ridge, test.data$spend),
Rsquare = R2(pred.ridge, test.data$spend)
)
# Let's do the same as before, but now with a continous outcome
knnr <- train(
spend ~. - type, data = train.data, #<<
method = "knn",
trControl = trainControl("cv", number = 10),
preProcess = c("center","scale"),
tuneLength = 50 #Again, play around with this parameter.
)
plot(knnr) #Exercise: Interpret this plot. What is on the Y axis? Is it better to have high values of Y or low?
# Let's test the error rates:
pred.spend <- knnr %>% predict(test.data)
RMSE(pred.spend, test.data$spend)
d <- read.csv("https://raw.githubusercontent.com/maibennett/sta235/main/exampleSite/content/Classes/Week11/2_KNearest/data/purchases_type.csv")
set.seed(100)
n <- nrow(d)
train.row <- sample(1:n, 0.8*n) # Same thing as before. We need to divide our data
test.data <- d[-train.row,]
train.data <- d[train.row,]
knn <- train(
type ~., data = train.data, # `type` is a factor variable now
method = "knn", # The method will be KNN
trControl = trainControl("cv", number = 10), # cross-validation with 10 fold
preProcess = c("center","scale"), # Again, we need to center and scale because we will be working with distances! (we want all distances to mean the same, meaning be in the same scale)
tuneLength = 15 # Play around with this parameter: Is the granularity for the search of the best K
)
# Exercise: Omit the tuneLength parameter. What do you see when you do plot(knn)? Increase tuneLength=35. What are the differences now?
plot(knn)
#Exercise: Interpret this plot!
pred.type <- knn %>% predict(test.data) #Use the testing data for prediction
table(pred.type, test.data$type) # This table shows the predicted types (rows) and the actual types (columns). You want the diagonals to have the most observations and hopefully the off-diagonals to be 0. Why?
## Regression
# Let's do the same as before, but now with a continous outcome
knnr <- train(
spend ~. - type, data = train.data, #<<
method = "knn",
trControl = trainControl("cv", number = 10),
preProcess = c("center","scale"),
tuneLength = 50 #Again, play around with this parameter.
)
plot(knnr) #Exercise: Interpret this plot. What is on the Y axis? Is it better to have high values of Y or low?
# Let's test the error rates:
pred.spend <- knnr %>% predict(test.data)
RMSE(pred.spend, test.data$spend)
knn$bestTune
################################################################################
### Title: "Week 11 - Shrinkage and KNN"
### Course: STA 235
### Semester: Spring 2021
### Professor: Magdalena Bennett
################################################################################
# Clears memory
rm(list = ls())
# Clears console
cat("\014")
### Load libraries
# If you don't have one of these packages installed already, you will need to
#run install.packages() line
library(tidyverse)
library(ggplot2)
library(broom)
library(caret)
############################################
#### Shrinkage
############################################
#### Rdige regression
data <- read.csv("https://raw.githubusercontent.com/maibennett/sta235/main/exampleSite/content/Classes/Week11/1_Shrinkage/data/purchases.csv") # Load data
set.seed(100) # Remember to set a seed!
train.row <- sample(1:n, 0.8*n) # We generate a random sample for the training data
test.data <- data[-train.row,] #Select testing data
train.data <- data[train.row,] #Select training data
## Exercise: Play around with the proportions. What happens if you hold-out just 10% of the sample? (i.e. use only 10% of the sample for testing). How do your results change?
lambda_seq <- c(0,10^seq(-3, 3, length = 100)) # Create a vector for different values of lambda
## Question: Why do we use an exponential vector?
cv <- train(spend ~., data = train.data, # We give the function `train` the formula (`spend` regressed on everything else that's on the dataset). Note: If there are variables *you don't want to include* (e.g. an id variable), you can exlude it with a - sign (e.g. spend ~ . - id).
method = "glmnet", # We will be using an "Elastic Net" method (which is basically the method family for ridge and lasso reg)
preProcess = "scale", # Pre-process the data (remember that lasso and ridge are affected by scales!)
trControl = trainControl("cv", number = 10), # We are using a cross-validation method, with 10 folds. What happens if you change the number of folds?
tuneGrid = expand.grid(alpha = 0, # alpha is the "mixing" parameter. All you need to know is that `alpha=0` is for ridge regression (and `alpha=1` is for lasso)
lambda = lambda_seq) # values of lambda we are going to test out
)
lambda <- cv$results$lambda #We can extract the vector of tested lambas here
rmse <- cv$results$RMSE # We can extract the C-V RMSE for each of those lambdas!
## Exercise: What other important things do you see in the matrix `results`? Explore it!
# I create a new dataframe to be able to plot lambda against RMSE
cv_lambda <- data.frame(lambda = lambda,
rmse = rmse)
# Plot lambda vs RMSE
ggplot(data = cv_lambda, aes(x = lambda, y = rmse)) +
geom_line(aes(color = "#BF3984"), lwd = 2) +
geom_vline(aes(xintercept = cv$bestTune$lambda, color = "dark grey"), lty=2, lwd = 1.5) +
scale_color_manual(values=c("#BF3984","dark grey"),labels = c("#BF3984" = "CV-RMSE","dark grey" = "Min \u03BB")) +
theme_bw()+
xlab("\u03BB") + ylab("RMSE")+
theme(plot.margin=unit(c(1,1,1.5,1.2),"cm"),
panel.grid.major.x = element_blank(),
panel.grid.minor.x = element_blank(),
panel.grid.major.y = element_blank(),
panel.grid.minor.y = element_blank(),
axis.line = element_line(colour = "dark grey"))+
theme(axis.title.x = element_text(size=18),#margin = margin(t = 10, r = 0, b = 0, l = 0)),
axis.text.x = element_text(size=14),
axis.title.y = element_text(size=18),#margin = margin(t = 0, r = 10, b = 0, l = 0)),
axis.text.y = element_text(size=14),legend.position=c(0.9,0.1),
legend.title = element_blank(),
legend.text = element_text(size=15),
legend.background = element_rect(fill="white",colour ="dark grey"),
title = element_text(size=14))
################################################################################
### Title: "Week 11 - Shrinkage and KNN"
### Course: STA 235
### Semester: Spring 2021
### Professor: Magdalena Bennett
################################################################################
# Clears memory
rm(list = ls())
# Clears console
cat("\014")
### Load libraries
# If you don't have one of these packages installed already, you will need to
#run install.packages() line
library(tidyverse)
library(ggplot2)
library(broom)
library(caret)
############################################
#### Shrinkage
############################################
#### Rdige regression
data <- read.csv("https://raw.githubusercontent.com/maibennett/sta235/main/exampleSite/content/Classes/Week11/1_Shrinkage/data/purchases.csv") # Load data
set.seed(100) # Remember to set a seed!
n <- nrow(data)
train.row <- sample(1:n, 0.8*n) # We generate a random sample for the training data
test.data <- data[-train.row,] #Select testing data
train.data <- data[train.row,] #Select training data
## Exercise: Play around with the proportions. What happens if you hold-out just 10% of the sample? (i.e. use only 10% of the sample for testing). How do your results change?
lambda_seq <- c(0,10^seq(-3, 3, length = 100)) # Create a vector for different values of lambda
## Question: Why do we use an exponential vector?
cv <- train(spend ~., data = train.data, # We give the function `train` the formula (`spend` regressed on everything else that's on the dataset). Note: If there are variables *you don't want to include* (e.g. an id variable), you can exlude it with a - sign (e.g. spend ~ . - id).
method = "glmnet", # We will be using an "Elastic Net" method (which is basically the method family for ridge and lasso reg)
preProcess = "scale", # Pre-process the data (remember that lasso and ridge are affected by scales!)
trControl = trainControl("cv", number = 10), # We are using a cross-validation method, with 10 folds. What happens if you change the number of folds?
tuneGrid = expand.grid(alpha = 0, # alpha is the "mixing" parameter. All you need to know is that `alpha=0` is for ridge regression (and `alpha=1` is for lasso)
lambda = lambda_seq) # values of lambda we are going to test out
)
lambda <- cv$results$lambda #We can extract the vector of tested lambas here
rmse <- cv$results$RMSE # We can extract the C-V RMSE for each of those lambdas!
## Exercise: What other important things do you see in the matrix `results`? Explore it!
# I create a new dataframe to be able to plot lambda against RMSE
cv_lambda <- data.frame(lambda = lambda,
rmse = rmse)
# Plot lambda vs RMSE
ggplot(data = cv_lambda, aes(x = lambda, y = rmse)) +
geom_line(aes(color = "#BF3984"), lwd = 2) +
geom_vline(aes(xintercept = cv$bestTune$lambda, color = "dark grey"), lty=2, lwd = 1.5) +
scale_color_manual(values=c("#BF3984","dark grey"),labels = c("#BF3984" = "CV-RMSE","dark grey" = "Min \u03BB")) +
theme_bw()+
xlab("\u03BB") + ylab("RMSE")+
theme(plot.margin=unit(c(1,1,1.5,1.2),"cm"),
panel.grid.major.x = element_blank(),
panel.grid.minor.x = element_blank(),
panel.grid.major.y = element_blank(),
panel.grid.minor.y = element_blank(),
axis.line = element_line(colour = "dark grey"))+
theme(axis.title.x = element_text(size=18),#margin = margin(t = 10, r = 0, b = 0, l = 0)),
axis.text.x = element_text(size=14),
axis.title.y = element_text(size=18),#margin = margin(t = 0, r = 10, b = 0, l = 0)),
axis.text.y = element_text(size=14),legend.position=c(0.9,0.1),
legend.title = element_blank(),
legend.text = element_text(size=15),
legend.background = element_rect(fill="white",colour ="dark grey"),
title = element_text(size=14))
## Exercise: Use the function `filter` from dplyr to zoom into lambda, and see the more interesting part (like we did in class!)
# Check out `bestTune` in the cv object: That will have the lambda that yields the smallest RMSE!
cv$bestTune$lambda
# Finally, check out the final model:
## Exercise: Interpret these results
coef(cv$finalModel, cv$bestTune$lambda)
# Remember that predictions need to be made on testing data!
pred.ridge <- cv %>% predict(test.data)
# Model prediction performance
data.frame(
RMSE = RMSE(pred.ridge, test.data$spend),
Rsquare = R2(pred.ridge, test.data$spend)
)
#### Lasso
## Exercise: Repeat the same steps for Lasso (remember that now `alpha=1`).
cvl <- train(spend ~., data = train.data,
method = "glmnet",
trControl = trainControl("cv", number = 10),
tuneGrid = expand.grid(alpha = 1, #<<
lambda = lambda_seq)
)
## Classifier
d <- read.csv("https://raw.githubusercontent.com/maibennett/sta235/main/exampleSite/content/Classes/Week11/2_KNearest/data/purchases_type.csv")
set.seed(100)
n <- nrow(d)
train.row <- sample(1:n, 0.8*n) # Same thing as before. We need to divide our data
test.data <- d[-train.row,]
train.data <- d[train.row,]
knn <- train(
type ~., data = train.data, # `type` is a factor variable now
method = "knn", # The method will be KNN
trControl = trainControl("cv", number = 10), # cross-validation with 10 fold
preProcess = c("center","scale"), # Again, we need to center and scale because we will be working with distances! (we want all distances to mean the same, meaning be in the same scale)
tuneLength = 15 # Play around with this parameter: Is the granularity for the search of the best K
)
# Exercise: Omit the tuneLength parameter. What do you see when you do plot(knn)? Increase tuneLength=35. What are the differences now?
plot(knn)
# You can also find the opt K:
knn$bestTune
#Exercise: Interpret this plot!
pred.type <- knn %>% predict(test.data) #Use the testing data for prediction
table(pred.type, test.data$type) # This table shows the predicted types (rows) and the actual types (columns). You want the diagonals to have the most observations and hopefully the off-diagonals to be 0. Why?
## Regression
# Let's do the same as before, but now with a continous outcome
knnr <- train(
spend ~. - type, data = train.data, #<<
method = "knn",
trControl = trainControl("cv", number = 10),
preProcess = c("center","scale"),
tuneLength = 50 #Again, play around with this parameter.
)
plot(knnr) #Exercise: Interpret this plot. What is on the Y axis? Is it better to have high values of Y or low?
#Question: What's the best K in this case?
# Let's test the error rates:
pred.spend <- knnr %>% predict(test.data)
RMSE(pred.spend, test.data$spend)
# Clears memory
rm(list = ls())
# Clears console
cat("\014")
### Load libraries
# If you don't have one of these packages installed already, you will need to
#run install.packages() line
library(tidyverse)
library(ggplot2)
library(broom)
library(caret)
############################################
#### Shrinkage
############################################
#### Rdige regression
data <- read.csv("https://raw.githubusercontent.com/maibennett/sta235/main/exampleSite/content/Classes/Week11/1_Shrinkage/data/purchases.csv") # Load data
set.seed(100) # Remember to set a seed!
n <- nrow(data)
train.row <- sample(1:n, 0.8*n) # We generate a random sample for the training data
test.data <- data[-train.row,] #Select testing data
train.data <- data[train.row,] #Select training data
# Clears memory
rm(list = ls())
# Clears console
cat("\014")
### Load libraries
# If you don't have one of these packages installed already, you will need to
#run install.packages() line
library(tidyverse)
library(ggplot2)
library(broom)
library(caret)
############################################
#### Shrinkage
############################################
#### Rdige regression
data <- read.csv("https://raw.githubusercontent.com/maibennett/sta235/main/exampleSite/content/Classes/Week11/1_Shrinkage/data/purchases.csv") # Load data
set.seed(100) # Remember to set a seed!
n <- nrow(data)
train.row <- sample(1:n, 0.8*n) # We generate a random sample for the training data
test.data <- data[-train.row,] #Select testing data
train.data <- data[train.row,] #Select training data
texas <- read.csv("https://raw.githubusercontent.com/maibennett/sta235/main/exampleSite/content/Assignments/Homework3/data/texas10.csv")
library(rdrobust)
library(rddensity)
library(tidyverse)
texas <- texas %>% mutate(dist = 0.1-ranking)
# a)
# We use distance for simplicity. Also, because rdplot() assumes treatment is <c
rdplot(texas$top10, texas$dist)
#b) We check density
dens <- rddensity(texas$dist)
summary(dens)
rdplotdensity(dens, texas$dist)
# c) Use hh_income and female
rdplot(texas$hh_income, texas$dist)
summary(rdrobust(texas$hh_income, texas$dist))
rdplot(texas$female, texas$dist)
summary(rdrobust(texas$female, texas$dist))
library(broom)
lm1 <- lm(earnings ~ dist + top10 + dist*top10, data = texas %>% filter(dist>=-0.05 & dist<=0.05))
summary(lm1)
texas_close <- augment(lm1, data = texas %>% filter(dist>=-0.05 & dist<=0.05))
library(ggplot2)
ggplot(data = texas_close, aes(x = dist, y = earnings)) +
geom_point(size = 3, pch = 21, color = "white", fill = "light grey") +
geom_line(data = texas_close %>% filter(dist<0),aes(x = dist, y = .fitted),
col = "#900DA4", lwd = 2) +
geom_line(data = texas_close %>% filter(dist>=0),aes(x = dist, y = .fitted),
col = "#F89441", lwd = 2) +
theme_bw()
ggplot(data = texas_close, aes(x = dist, y = earnings)) +
geom_point(size = 3, pch = 21, color = "white", fill = "light grey") +
geom_line(data = texas_close %>% filter(dist<0),aes(x = ranking, y = .fitted),
col = "#900DA4", lwd = 2) +
geom_line(data = texas_close %>% filter(dist>=0),aes(x = ranking, y = .fitted),
col = "#F89441", lwd = 2) +
theme_bw()
ggplot(data = texas_close, aes(x = ranking, y = earnings)) +
geom_point(size = 3, pch = 21, color = "white", fill = "light grey") +
geom_line(data = texas_close %>% filter(dist<0),aes(x = ranking, y = .fitted),
col = "#900DA4", lwd = 2) +
geom_line(data = texas_close %>% filter(dist>=0),aes(x = ranking, y = .fitted),
col = "#F89441", lwd = 2) +
theme_bw()
ggplot(data = texas_close, aes(x = ranking, y = earnings)) +
geom_point(size = 3, pch = 21, color = "white", fill = "light grey") +
geom_line(data = texas_close %>% filter(ranking<0.1),aes(x = ranking, y = .fitted),
col = "#900DA4", lwd = 2) +
geom_line(data = texas_close %>% filter(ranking>=0.1),aes(x = ranking, y = .fitted),
col = "#F89441", lwd = 2) +
theme_bw()
ggplot(data = texas_close, aes(x = ranking, y = earnings)) +
geom_point(size = 3, pch = 21, color = "white", fill = "light grey") +
geom_line(data = texas_close %>% filter(ranking<=0.1),aes(x = ranking, y = .fitted),
col = "#900DA4", lwd = 2) +
geom_line(data = texas_close %>% filter(ranking>0.1),aes(x = ranking, y = .fitted),
col = "#F89441", lwd = 2) +
theme_bw()
ggplot(data = texas_close, aes(x = ranking, y = earnings)) +
geom_point(size = 3, pch = 21, color = "white", fill = "light grey") +
geom_line(data = texas_close %>% filter(ranking<=0.1),aes(x = ranking, y = .fitted),
color = "#900DA4", lwd = 2) +
geom_line(data = texas_close %>% filter(ranking>0.1),aes(x = ranking, y = .fitted),
color = "#F89441", lwd = 2) +
theme_bw()
ggplot(data = texas_close, aes(x = ranking, y = earnings)) +
geom_point(size = 3, pch = 21, color = "white", fill = "light grey") +
geom_line(data = dplyr::filter(texas_close,ranking>0.1),aes(x = ranking, y = .fitted),
color = "#900DA4", lwd = 2) +
geom_line(data = dplyr::filter(texas_close,ranking<=0.1),aes(x = ranking, y = .fitted),
color = "#F89441", lwd = 2) +
theme_bw()
texas_close <- texas %>% dplyr::filter(dist >+ -0.05 & dist<=0.05)
lm1_close <- lm(earnings ~ dist + top10 + dist*top10, data = texas_close)
summary(lm1_close)
texas_aug <- broom::augment(lm1_close, data = texas_close)
ggplot(data = texas_aug, aes(x = ranking, y = earnings)) +
geom_point(size = 3, pch = 21, color = "white", fill = "grey") +
geom_line(data = dplyr::filter(texas_aug, ranking>0.1),aes(x = ranking, y = .fitted),
col = "#900DA4", lwd = 2) +
geom_line(data = dplyr::filter(texas_aug, ranking<=0.1),aes(x = ranking, y = .fitted),
col = "#F89441", lwd = 2) +
theme_bw()
pagedown::chrome_print('C:/Users/mc72574/Dropbox/Hugo/Sites/sta235/exampleSite/content/Classes/Week11/1_shrinkage/sp2021_sta235_13_shrinkage.html', timeout = 10)
pagedown::chrome_print('C:/Users/mc72574/Dropbox/Hugo/Sites/sta235/exampleSite/content/Classes/Week11/2_KNearest/sp2021_sta235_14_knn.html', timeout = 10)
