<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>STA 235 - Causal Inference: Introduction to Observational Studies</title>
    <meta charset="utf-8" />
    <meta name="author" content="McCombs School of Business, UT Austin" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <script src="https://use.fontawesome.com/5235085b15.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="custom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# STA 235 - Causal Inference: Introduction to Observational Studies
## Spring 2021
### McCombs School of Business, UT Austin

---





&lt;style type="text/css"&gt;

.small .remark-code { /*Change made here*/
  font-size: 80% !important;
}

.tiny .remark-code { /*Change made here*/
  font-size: 90% !important;
}
&lt;/style&gt;







# Last class

.pull-left[
- **.darkorange[Randomized controlled trials]**
  
  - Why is it considered the gold standard?
  - How to analyze an RCT in practice?
  - Assumptions and limitations.
  
- **.darkorange[Statistical power]**:
  
  - How do sample sizes play a role?
  - Randomized inference.
]

.pull-right[
.center[
![:scale 80%](https://media.giphy.com/media/jsN192JGdyWvS1gqTb/giphy.gif)]
]


---
# Today

.pull-left[
.center[
![](https://github.com/maibennett/sta235/raw/main/exampleSite/content/Classes/Week5/1_Matching/images/data.jpg)]
]
.pull-right[
- **.darkorange[Introduction to Observational Studies]**:
  
  - Can we identify causal effects without RCTs?
  
  - Assumptions
  
  - Matching vs OLS
]

---
background-position: 50% 50%
class: left, bottom, inverse
.big[
No more chance[s]
]


---
# Introduction to observational studies

- Most times, we will not be able to randomize, and we need to work with **.darkorange[existing data]**

--
&lt;br&gt;
&lt;br&gt;

.box-5LA[Observational data]

--

- Data for which we can't manipulate the treatment assignment, e.g. data in its "natural state".

--
&lt;br&gt;
&lt;br&gt;

.box-7[Can we reasonably assume that the ignorability assumption holds?]

---
# Introduction to observational studies (cont.)

.pull-left[
.center[
.![:scale 80%](https://media.giphy.com/media/fu2rIw18eIDz1YCrCT/giphy.gif)
]
]

.pull-right[
- Moving away from the core assumption of RCTs: that **.darkorange["the probability of treatment assignment is a known function"]** (Imbens &amp; Rubin, 2015).

]

---
# Introduction to observational studies (cont.)

.pull-left[
.center[
.![:scale 80%](https://media.giphy.com/media/fu2rIw18eIDz1YCrCT/giphy.gif)
]
]

.pull-right[
- Moving away from the core assumption of RCTs: that **.darkorange["the probability of treatment assignment is a known function"]** (Imbens &amp; Rubin, 2015).

- We will maintain the assumption of **.darkorange[unconfoundnedness]** (to a certain extent).

.box-4t[What is that?]
]


---
# Calling in the CIA

- **.darkorange[Unconfoundnedness]** means that the treatment assignment is independent from the potential outcomes.

- If you recall, the ignorability assumption assumes that: 

`$$Y(0), Y(1) \perp\!\!\!\perp Z$$`

--

- What if you could assume that this holds **.darkorange[conditional on some covariates]**?

--

.box-2LA[Conditional Independence Assumption (CIA)]
&lt;br&gt;
`$$Y(0), Y(1) \perp\!\!\!\perp Z|X$$`

---
# The assignment mechanism

.pull-left[
- **.darkorange[Key component]** in causal analysis:

  - In RCTs, **.darkorange[assignment mechanism]** is *known*.
  
  - But in **.darkorange[observational studies?]**

]
.pull-right[
.center[
![:scale 70%](https://github.com/maibennett/sta235/raw/main/exampleSite/content/Classes/Week5/1_Matching/images/confusion.png)]
]

---
# The assignment mechanism (cont.)

- For now, we will consider 3 assumptions:

--

.box-2[Individualistic assignment]

--

.box-4[Probabilistic assignment]

--

.box-7[Unconfounded assignment]


---
# Individualistic assignment

- Limits the dependence of treatment assignment for unit `\(i\)` on the outcomes and assignments for other units.

- For some function `\(q(\cdot) \in [0,1]\)`:

`$$p_i(\mathbf{X},\mathbf{Y}(0),\mathbf{Y}(1)) = q(X_i,Y_i(0),Y_i(1))$$`
--
&lt;br&gt;
&lt;br&gt;
.box-6LA[From last class, can you think of an example where this doesn't hold?]

---
# Probabilistic assignment

- Nonzero probability for each treatment value, for each unit.

`$$0 &lt; p_i(\mathbf{X},\mathbf{Y}(0),\mathbf{Y}(1)) &lt; 1 \ \ \forall \ i = 1,...,N$$`
  ,for each possible `\(\mathbf{X},\mathbf{Y}(0),\mathbf{Y}(1)\)`
  
--
&lt;br&gt;
&lt;br&gt;
.box-4LA[When could this assumption fail?]
---
#Unconfounded assignment

- The assignment mechanism does not depend on the potential outcomes:

`$$Pr(\mathbf{Z}|\mathbf{X},\mathbf{Y}(0),\mathbf{Y}(1)) = Pr(\mathbf{Z}|\mathbf{X},\mathbf{Y'}(0),\mathbf{Y'}(1))$$`
  ,for all possible `\(\mathbf{Z}, \mathbf{X},\mathbf{Y}(0),\mathbf{Y}(1), \mathbf{Y'}(0),\)` and `\(\mathbf{Y'}(1)\)`
  
---
# Selection on observables

- Units select into treatment based on characteristics **.darkorange[I can observe]**.

--
&lt;br&gt;
&lt;br&gt;
.center[
![](https://github.com/maibennett/sta235/raw/main/exampleSite/content/Classes/Week5/1_Matching/images/unconfound1.png)
]

---
# Selection on observables

- Units select into treatment based on characteristics **.darkorange[I can observe]**.
&lt;br&gt;
&lt;br&gt;
.center[
![](https://github.com/maibennett/sta235/raw/main/exampleSite/content/Classes/Week5/1_Matching/images/unconfound2.png)
]

---
# How do we adjust for observables?

- One way we have seen so far is **.darkorange[regression adjustment]**

`$$Y_i = \beta_0 + \beta_1 Z_i + \beta_2 X_i + \varepsilon_i$$`
--
&lt;br&gt;
&lt;br&gt;
.box-2[Under unconfoundedness, how would we interpret &amp;beta;&lt;sub&gt;1&lt;/sub&gt;?]

---
# How do we adjust for observables?

- One way we have seen so far is **.darkorange[regression adjustment]**

`$$Y_i = \beta_0 + \beta_1 Z_i + \beta_2 X_i + \varepsilon_i$$`

&lt;br&gt;
&lt;br&gt;
.box-2[&amp;beta;&lt;sub&gt;1&lt;/sub&gt; is the effect of Z on Y, holding X constant]

---
# How do we adjust for observables?

- But what if our data looks like this? Do you see a problem?

&lt;img src="sp2021_sta235_7_matching_files/figure-html/extrapolate_ols1-1.svg" style="display: block; margin: auto;" /&gt;
---
# How do we adjust for observables?

- But what if our data looks like this? Do you see a problem?

&lt;img src="sp2021_sta235_7_matching_files/figure-html/extrapolate_ols2-1.svg" style="display: block; margin: auto;" /&gt;

---
# How do we adjust for observables?

- But what if our data looks like this? Do you see a problem?

&lt;img src="sp2021_sta235_7_matching_files/figure-html/extrapolate_ols3-1.svg" style="display: block; margin: auto;" /&gt;

---
background-position: 50% 50%
class: left, bottom, inverse
.big[
Finding your perfect match...
]

---
# Two peas in a pod

- One other route we could take is to **.darkorange[find similar units]** in our sample and **.darkorange[group them together]**.

--

.pull-left[
- There are different ways to do it:

  - E.g. subclassification, matching.
]
.pull-right[
![](https://media.giphy.com/media/l36kU80xPf0ojG0Erg/giphy.gif)
]

---
# Two peas in a pod

- One other route we could take is to **.darkorange[find similar units]** in our sample and **.darkorange[group them together]**.



.pull-left[
- There are different ways to do it:

  - E.g. subclassification, matching.
  
.box-2t[What do we gain?]  
]
.pull-right[
![](https://media.giphy.com/media/l36kU80xPf0ojG0Erg/giphy.gif)
]

---
# Advantages of matching methods

.box-2[Reduce model dependence]
--
&lt;br&gt;
.box-2t[Imbalance → model dependence → researcher discretion → bias]

--

.box-4[Compare like to like]
--
&lt;br&gt;
.box-4t[No extrapolation!]

--

.box-7[Can adjust closely by covariates]
--
&lt;br&gt;
.box-7t[Exact matching, coarsened exact matching, fine balance..]

---

$$
\color{white}{\beta_0 \text{E}^2}
$$


&lt;img src="sp2021_sta235_7_matching_files/figure-html/matching-general-1.svg" style="display: block; margin: auto;" /&gt;

---
$$
\color{white}{\beta_0 \text{E}^2} \text{Outcome} = \beta_0 + \beta_1 \text{Education} + \beta_2 \text{Treatment} \color{white}{\beta_0 \text{E}^2}
$$


&lt;img src="sp2021_sta235_7_matching_files/figure-html/matching-dependency1-1.svg" style="display: block; margin: auto;" /&gt;

---
$$
\text{Outcome} = \beta_0 + \beta_1 \text{Education} + \beta_2 \text{Education}^2 + \beta_3 \text{Treatment}
$$


&lt;img src="sp2021_sta235_7_matching_files/figure-html/dependency2-1.svg" style="display: block; margin: auto;" /&gt;

---
$$
\color{white}{\beta_0 \text{E}^2}
$$


&lt;img src="sp2021_sta235_7_matching_files/figure-html/reduced-1.svg" style="display: block; margin: auto;" /&gt;

---
$$
\color{white}{\beta_0 \text{E}^2} \text{Outcome} = \beta_0 + \beta_1 \text{Education} + \beta_2 \text{Treatment} \color{white}{\beta_0 \text{E}^2}
$$


&lt;img src="sp2021_sta235_7_matching_files/figure-html/reduced-dependency1-1.svg" style="display: block; margin: auto;" /&gt;

---
$$
\text{Outcome} = \beta_0 + \beta_1 \text{Education} + \beta_2 \text{Education}^2 + \beta_3 \text{Treatment}
$$


&lt;img src="sp2021_sta235_7_matching_files/figure-html/reduced-dependency2-1.svg" style="display: block; margin: auto;" /&gt;

---
.box-4[How do we know we can remove those observations?]

&lt;img src="sp2021_sta235_7_matching_files/figure-html/reduced2-1.svg" style="display: block; margin: auto;" /&gt;
---
# Subclassification

.pull-left[
- Very similar to **.darkorange[stratifying]**.

]

.pull-right[
&lt;img src="sp2021_sta235_7_matching_files/figure-html/subclass1-1.svg" style="display: block; margin: auto;" /&gt;
]

---
# Subclassification

.pull-left[
- Very similar to **.darkorange[stratifying]**.

- Build **.darkorange[combination]** of X1 and X2 (strata).
]

.pull-right[
&lt;img src="sp2021_sta235_7_matching_files/figure-html/subclass2-1.svg" style="display: block; margin: auto;" /&gt;
]

---
# Subclassification

.pull-left[
- Very similar to **.darkorange[stratifying]**.

- Build **.darkorange[combination]** of X1 and X2 (strata).

- Compare **.darkorange[within stratum]**.
]

.pull-right[
&lt;img src="sp2021_sta235_7_matching_files/figure-html/subclass3-1-1.svg" style="display: block; margin: auto;" /&gt;
]

---
# Subclassification

.pull-left[
- Very similar to **.darkorange[stratifying]**.

- Build **.darkorange[combination]** of X1 and X2 (strata).

- Compare **.darkorange[within stratum]**.
]

.pull-right[
&lt;img src="sp2021_sta235_7_matching_files/figure-html/subclass3-2-1.svg" style="display: block; margin: auto;" /&gt;
]

---
# Subclassification

.pull-left[
- Very similar to **.darkorange[stratifying]**.

- Build **.darkorange[combination]** of X1 and X2 (strata).

- Compare **.darkorange[within stratum]**.
]

.pull-right[
&lt;img src="sp2021_sta235_7_matching_files/figure-html/subclass3-3-1.svg" style="display: block; margin: auto;" /&gt;
]

---
# Subclassification

.pull-left[
- Very similar to **.darkorange[stratifying]**.

- Build **.darkorange[combination]** of X1 and X2 (strata).

- Compare **.darkorange[within stratum]**.
]

.pull-right[
&lt;img src="sp2021_sta235_7_matching_files/figure-html/subclass3-4-1.svg" style="display: block; margin: auto;" /&gt;
]

---
# Subclassification

.pull-left[
- Very similar to **.darkorange[stratifying]**.

- Build **.darkorange[combination]** of X1 and X2 (strata).

- Compare **.darkorange[within stratum]**.
]

.pull-right[
&lt;img src="sp2021_sta235_7_matching_files/figure-html/subclass3-5-1.svg" style="display: block; margin: auto;" /&gt;
]

---
# Subclassification

.pull-left[
- Very similar to **.darkorange[stratifying]**.

- Build **.darkorange[combination]** of X1 and X2 (strata).

- Compare **.darkorange[within stratum]**.

]

.pull-right[
&lt;img src="sp2021_sta235_7_matching_files/figure-html/subclass3-6-1.svg" style="display: block; margin: auto;" /&gt;
]

---
# Subclassification

- To estimate the Average Treatment Effect, we take a **.darkorange[weighted average]**:

`$$\hat{ATE}=\sum_{s=1}^S\frac{N_s}{N}(\bar{Y}_{1s}-\bar{Y}_{0s})$$`
--

.box-3[What happens when we have too many variables to build strata?]

---
# The curse of dimentionality

.pull-left[
- When we have too many covariates, the number of strata or groups grow **.darkorange[exponentially]**!

  - E.g. with 4 covariates, each with 5 categories, we have **.darkorange[625 combinations]**!

- Very possible that a stratum only has treatment or control units.
]

.pull-right[
.center[
![:scale 80%](https://media.giphy.com/media/eZkKm90RRqfHrL1MUe/giphy.gif)
]
]

---
# The curse of dimentionality

.pull-left[
- When we have too many covariates, the number of strata or groups grow **.darkorange[exponentially]**!

  - E.g. with 4 covariates, each with 5 categories, we have **.darkorange[625 combinations]**!

- Very possible that a stratum only has treatment or control units.

.box-5t[What to do?]
]

.pull-right[
.center[
![:scale 80%](https://media.giphy.com/media/eZkKm90RRqfHrL1MUe/giphy.gif)
]
]

---
# Breaking the curse: Balancing scores

- Want to **.darkorange[reduce the dimentionality]** of our covariates

--
&lt;br&gt;
&lt;br&gt;
- A balancing score `\(b(x)\)` is a function of the covariates such that:

`$$Z_i \perp\!\!\!\perp X_i|b(X_i)$$`
--

- This means that conditioning on the balancing score is **.darkorange[enough to remove bias]** associated to the covariates.

- Under unconfoundedness:

`$$Z_i \perp\!\!\!\perp Y_i(0),Y_i(1)|b(X_i)$$`

--

- There are different balancing scores:

  - E.g. propensity scores, mahalanobis distance.
  
---
# Estimating balancing scores

.box-5LA[Propensity score]

`$$\log(\frac{p}{1-p}) = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_p X_p + \varepsilon$$`
where `\(p = Pr(Z=1)\)`

--

```r
e &lt;- predict(glm(z ~ x1 + x2 + x3, data = d, family = binomial(link="logit")),
             type="response")
```

---
# Estimating balancing scores

.box-5LA[Propensity score]

- Importance of overlap region

--
&lt;img src="sp2021_sta235_7_matching_files/figure-html/overlap-1.svg" style="display: block; margin: auto;" /&gt;


---
# Estimating balancing scores

.box-7LA[Mahalanobis Distance]

$$D_M(\mathbf{x}) = \sqrt{(\mathbf{x}-\mathbf{\mu})^TS^{-1}(\mathbf{x}-\mathbf{\mu})} $$
where `\(\mathbf{x}\)` is the covariate vector for observation `\(i\)`, `\(\mathbf{\mu}\)` is the mean vector of covariates for the sample, and `\(S\)` is the covariance matrix. 

--

```r
Sx &lt;- cov(x)
D &lt;- mahalanobis(x, colMeans(x), Sx)

# We can also use a rank-based Mahalanobis distance matrix
library(designmatch)

D &lt;- distmat(z, x)
```

---
# Making groups comparable

- Using the previous balancing scores (or covariates directly!) we can **.darkorange[match observations between the treatment and control group]**

--

.box-4[Step 1: Preprocessing]

.box-4t[Try to model the treatment assignment]

--

.box-5[Step 2: Estimation]

.box-5t[Use the new trimmed/preprocessed data to build a model, calculate difference in means, etc.]

---
# How matchy-matchy

- There are different matching methods (and different ways to use them!)

--

.box-2[Nearest neighbor (NN)]

.box-2t[Use balancing scores; Greedy algorithm]

--

.box-4[Optimal matching]

.box-4t[Solves an optimization problem; slow on large samples]

--

.box-7[Mixed Integer Programming (MIP) matching]

.box-7t[Balances covariates directly; can generate smaller samples]

---

.center2[
.box-5LA[Let's go to R]
]

---
# Propensity score matching

.box-1[Super popular method]

--

.box-3[There are mathy reasons why it's not great&lt;br&gt;for matching *for identification purposes*]

--

.box-7[Some researchers argue propensity scores are fine!&lt;br&gt;Using them for matching isn't!]

---

.center[
![:scale 100%](https://github.com/maibennett/sta235/raw/main/exampleSite/content/Classes/Week5/1_Matching/images/gary-king.png)
]

---
# Weighting, the cousin of matching

- We can also use **.darkorange[weights]** to make two samples look alike.

--
&lt;br&gt;
&lt;br&gt;
.box-4[Inverse Probability Weighting (IPW)]

- Make some observations more important than others.

---
# Weighting, the cousin of matching

- You can make the treatment and the control group look more like each other:

`$$w_{ATE} = \frac{Z_i}{e_i} + \frac{1-Z_i}{1-e_i}$$`

- Observations in the control group will have a weight of `\(\frac{1}{1-e_i}\)`, while observations in the treatment group will have weights of `\(\frac{1}{e_i}\)`

--

.box-3[What happens with obs that are very likely to be in the treatment group?]

---
# Weighting, the cousin of matching

- You can make the treatment and the control group look more like each other:

`$$w_{ATT} = \frac{e_i\cdot Z_i}{e_i} + \frac{e_i(1-Z_i)}{1-e_i}$$`

- Observations in the control group will have a weight of `\(\frac{e_i}{1-e_i}\)`, while observations in the treatment group will have weights of 1

--

.box-5[Why do observations in the treatment group get a weight of 1?]

---
# Weighting for approximating a population

- If we assume that our sample was selected on observables, and we want it to **.darkorange[look more like a population of interest]**, we can also do that!

--

- We can use the same formula for `\(ATT\)`, but now `\(e_i\)` is not the probability of being assigned to treatment, but the probability of **.darkorange[being in the population sample]**.

--

- For this type of weighting, we only need population covariates.

---

.center2[
.box-3LA[Let's go to R]
]

---
# The shortcomings of matching

- Many researchers missuse matching and **.darkorange[confuse it with an identification strategy]**

--

- In terms of identification, **.darkorange[matching still relies on selection on observables]**

--

.box-2[You need other source of exogeneous variation!]

--

- Claiming that you can identify a **.darkorange[causal effect]** just by using matching is almost the same as claiming this using a regression approach.

.box-7[Not a good idea...]

---
# Don't get it twisted

.pull-left[

- Matching works great as an **.darkorange[adjustment method]**.

- Combined with **.darkorange[other identification strategies]**, it can improve results!

]
.pull-right[
.center[
![](https://media.giphy.com/media/kBNhZkdEPOgoB8p2nT/giphy.gif)
]
]

---
# Main takeaways

.pull-left[
.center[
![](https://media.giphy.com/media/xThtalMViID4iGemWI/giphy-downsized-large.gif)
]
]
.pull-right[
- Matching and weighting methods can be great tools for your analysis.

  - Create more similar groups of comparisons.

  - Reduce model dependence
  
  - Even help with external validity (under assumptions)
]

---
# Next week

- We will look at some **.darkorange[identification strategies]** for observational studies:

  - Natural experiments and differences-in-differences.
  
- What **.darkorange[assumptions]** need to hold?

- How do we **.darkorange[identify a natural experiment]**?

- What does **.darkorange[DD]** buy us?

---
# References

- Angrist, J. and S. Pischke. (2015). "Mastering Metrics". *Chapter 2*.

- Heiss, A. (2020). "Program Evaluation for Public Policy". *Class 7: Randomization and Matching, Course at BYU*

- Imbens, G. and D. Rubin. (2015). "Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction". *Chapter 3*

- Cunningham, S. (2021). "Causal Inference: The Mixtape". *Chapter 5*


&lt;!-- pagedown::chrome_print('C:/Users/mc72574/Dropbox/Hugo/Sites/sta235/exampleSite/content/Classes/Week4/1_RCT/sp2021_sta235_6_RCT.html') --&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script src="cols_macro.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
